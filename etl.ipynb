{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, IntegerType, LongType, ShortType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://udacity-dend-bucket-ny/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `song_data`\n",
    "In this first part, you'll perform ETL on the first dataset, `song_data`, to create the `songs` and `artists` dimensional tables.\n",
    "\n",
    "Let's perform ETL on a single song file and load a single record into each table to start.\n",
    "- Use the `get_files` function provided above to get a list of all song JSON files in `data/song_data`\n",
    "- Select the first song in this list\n",
    "- Read the song file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "\n",
    "#Testing\n",
    "song_data = './data/song_data/*/*/*/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "songSchema = StructType([\n",
    "    StructField(\"artist_id\", StringType()),\n",
    "    StructField(\"artist_latitude\", DoubleType()),\n",
    "    StructField(\"artist_location\", StringType()),\n",
    "    StructField(\"artist_longitude\", DoubleType()),\n",
    "    StructField(\"artist_name\", StringType()),\n",
    "    StructField(\"duration\", DoubleType()),\n",
    "    StructField(\"num_songs\", ShortType()),\n",
    "    StructField(\"song_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", ShortType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df = spark.read.json(song_data, schema=songSchema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file Testing\n",
    "#df1 = spark.read.json(song_data, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT DISTINCT song_id\n",
    "#                       , title\n",
    "#                       , artist_id\n",
    "#                       , year\n",
    "#                       , duration\n",
    "#            FROM staging_songs\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: `songs` Table\n",
    "#### Extract Data for Songs Table\n",
    "- Select columns for song ID, title, artist ID, year, and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = spark.sql('''\n",
    "                        SELECT DISTINCT song_id\n",
    "                                      , title\n",
    "                                      , artist_id\n",
    "                                      , year\n",
    "                                      , duration\n",
    "                          FROM staging_songs\n",
    "                         LIMIT 1\n",
    "                        '''\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode('overwrite').partitionBy('year', 'artist_id').parquet(output_data + 'songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    songs_table count: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"    songs_table count:\", songs_table.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Song Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(output_data + 'songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOBBXLX12A58A79DDA|Erica (2005 Digit...|138.63138|   0|AREDBBQ1187B98AFF5|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: `artists` Table\n",
    "#### Extract Data for Artists Table\n",
    "- Select columns for artist ID, name, location, latitude, and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = spark.sql('''\n",
    "                          SELECT DISTINCT artist_id\n",
    "                                        , artist_name AS name\n",
    "                                        , artist_location AS location\n",
    "                                        , artist_latitude AS lattitude\n",
    "                                        , artist_longitude AS longitude\n",
    "                            FROM staging_songs\n",
    "                           LIMIT 1\n",
    "                          '''\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------------+---------+---------+\n",
      "|         artist_id|    name|    location|lattitude|longitude|\n",
      "+------------------+--------+------------+---------+---------+\n",
      "|ARPBNLO1187FB3D52F|Tiny Tim|New York, NY| 40.71455|-74.00712|\n",
      "+------------------+--------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.mode('overwrite').parquet(output_data + 'artists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- lattitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(output_data + 'artists')\n",
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------------+---------+---------+\n",
      "|         artist_id|    name|    location|lattitude|longitude|\n",
      "+------------------+--------+------------+---------+---------+\n",
      "|ARPBNLO1187FB3D52F|Tiny Tim|New York, NY| 40.71455|-74.00712|\n",
      "+------------------+--------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `log_data`\n",
    "In this part, you'll perform ETL on the second dataset, `log_data`, to create the `time` and `users` dimensional tables, as well as the `songplays` fact table.\n",
    "- Filter records by `NextSong` action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + 'log_data/*/*/*.json'\n",
    "#Testing\n",
    "log_data = './data/log_data/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSchema = StructType([\n",
    "    StructField(\"artist\", StringType()),\n",
    "    StructField(\"auth\", StringType()),\n",
    "    StructField(\"firstName\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"itemInSession\", ShortType()),\n",
    "    StructField(\"lastName\", StringType()),\n",
    "    StructField(\"length\", DoubleType()),\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"method\", StringType()),\n",
    "    StructField(\"page\", StringType()),\n",
    "    StructField(\"registration\", DoubleType()), #Convert to Long before writing to parquet files\n",
    "    StructField(\"sessionId\", ShortType()),\n",
    "    StructField(\"song\", StringType()),\n",
    "    StructField(\"status\", ShortType()),\n",
    "    StructField(\"ts\", LongType()),\n",
    "    StructField(\"userAgent\", StringType()),\n",
    "    StructField(\"userId\", StringType()) #Convert to Short before writing to parquet files\n",
    "])\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data, schema=logSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: short (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: short (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: short (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = spark.read.json(log_data)\n",
    "#df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"staging_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT DISTINCT CAST(userId AS short)\n",
    "#            FROM staging_events\n",
    "#           LIMIT 10\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT DISTINCT CAST(registration AS long)\n",
    "#            FROM staging_events\n",
    "#           LIMIT 10\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT to_timestamp(ts/1000) as ts\n",
    "#               , EXTRACT(hour FROM to_timestamp(ts/1000)) AS hour\n",
    "#            FROM staging_events\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT DISTINCT ts\n",
    "#                        , EXTRACT(hour FROM ts) AS hour\n",
    "#                        , EXTRACT(day FROM ts) AS day\n",
    "#                        , EXTRACT(week FROM ts) AS week\n",
    "#                        , EXTRACT(month FROM ts) AS month\n",
    "#                        , EXTRACT(year FROM ts) AS year\n",
    "#                        , EXTRACT(dayofweek FROM ts) AS weekday\n",
    "#            FROM (SELECT to_timestamp(ts/1000) AS ts\n",
    "#                    FROM staging_events)\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT count(*)\n",
    "#            FROM staging_events\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT DISTINCT page\n",
    "#            FROM staging_events\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.udf.register(\"get_ts\", lambda x: datetime.fromtimestamp(x / 1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('''\n",
    "#          SELECT ts, CAST(get_ts(ts) AS int)\n",
    "#          FROM staging_events \n",
    "#          LIMIT 1\n",
    "#          '''\n",
    "#          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: `users` Table\n",
    "#### Extract Data for Users Table\n",
    "- Select columns for user ID, first name, last name, gender and level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "|      8|    Kaylee|  Summers|     F| free|\n",
      "|     17|  Makinley|    Jones|     F| free|\n",
      "|     82|     Avery| Martinez|     F| paid|\n",
      "|     36|   Matthew|    Jones|     M| paid|\n",
      "|     11| Christian|   Porter|     F| free|\n",
      "|      5|    Elijah|    Davis|     M| free|\n",
      "|      9|     Wyatt|    Scott|     M| free|\n",
      "|     14|  Theodore|   Harris|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT DISTINCT user_id\n",
    "                      , first_name\n",
    "                      , last_name\n",
    "                      , gender\n",
    "                      , level\n",
    "          FROM (SELECT ROW_NUMBER() OVER(PARTITION BY userId \n",
    "                                             ORDER BY ts DESC)\n",
    "                    AS row_num\n",
    "                     , CAST(userId AS short) AS user_id\n",
    "                     , firstName AS first_name\n",
    "                     , lastName AS last_name\n",
    "                     , gender\n",
    "                     , level\n",
    "                     , ts\n",
    "                  FROM staging_events)\n",
    "         WHERE row_num = 1\n",
    "         LIMIT 10\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table    \n",
    "users_table = spark.sql('''\n",
    "    SELECT DISTINCT user_id\n",
    "                  , first_name\n",
    "                  , last_name\n",
    "                  , gender\n",
    "                  , level\n",
    "      FROM (SELECT ROW_NUMBER() OVER(PARTITION BY userId \n",
    "                                         ORDER BY ts DESC)\n",
    "                AS row_num\n",
    "                 , CAST(userId AS short) AS user_id\n",
    "                 , firstName AS first_name\n",
    "                 , lastName AS last_name\n",
    "                 , gender\n",
    "                 , level\n",
    "                 , ts\n",
    "              FROM staging_events)\n",
    "     WHERE row_num = 1\n",
    "     LIMIT 1\n",
    "'''\n",
    ")\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode('overwrite').parquet(output_data + 'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: short (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(output_data + 'users')\n",
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: `time` Table\n",
    "#### Extract Data for Time Table\n",
    "- Convert the `ts` timestamp column to datetime\n",
    "  - Hint: the current timestamp is in milliseconds\n",
    "- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these values in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf()\n",
    "#df = \n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "#get_datetime = udf()\n",
    "#df = \n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = spark.sql('''\n",
    "    SELECT DISTINCT start_time\n",
    "                  , EXTRACT(hour FROM start_time) AS hour\n",
    "                  , EXTRACT(day FROM start_time) AS day\n",
    "                  , EXTRACT(week FROM start_time) AS week\n",
    "                  , EXTRACT(month FROM start_time) AS month\n",
    "                  , EXTRACT(year FROM start_time) AS year\n",
    "                  , EXTRACT(dayofweek FROM start_time) AS weekday\n",
    "      FROM (SELECT to_timestamp(ts/1000.0) AS start_time\n",
    "              FROM staging_events)\n",
    "     LIMIT 1\n",
    "'''\n",
    ")\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy('year', 'month').parquet(output_data + 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(output_data + 'time')\n",
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-------+----+-----+\n",
      "|          start_time|hour|day|week|weekday|year|month|\n",
      "+--------------------+----+---+----+-------+----+-----+\n",
      "|2018-11-15 21:04:...|  21| 15|  46|      5|2018|   11|\n",
      "+--------------------+----+---+----+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: `songplays` Table\n",
    "#### Extract Data and Songplays Table\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----+-----+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "|songplay_id|          start_time|year|month|user_id|level|song_id|artist_id|session_id|            location|          user_agent|\n",
      "+-----------+--------------------+----+-----+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "|          1|2018-11-01 21:01:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          2|2018-11-01 21:05:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          3|2018-11-01 21:08:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          4|2018-11-01 21:11:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          5|2018-11-01 21:17:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          6|2018-11-01 21:24:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          7|2018-11-01 21:28:...|2018|   11|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|\n",
      "|          8|2018-11-01 21:42:...|2018|   11|     10| free|   null|     null|         9|Washington-Arling...|\"Mozilla/5.0 (Mac...|\n",
      "|          9|2018-11-01 21:52:...|2018|   11|     26| free|   null|     null|       169|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|         10|2018-11-01 21:55:...|2018|   11|     26| free|   null|     null|       169|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|         11|2018-11-01 22:23:...|2018|   11|    101| free|   null|     null|       100|New Orleans-Metai...|\"Mozilla/5.0 (Win...|\n",
      "|         12|2018-11-02 01:25:...|2018|   11|    101| free|   null|     null|       184|New Orleans-Metai...|\"Mozilla/5.0 (Win...|\n",
      "|         13|2018-11-02 01:30:...|2018|   11|     83| free|   null|     null|        82|         Lubbock, TX|\"Mozilla/5.0 (Mac...|\n",
      "|         14|2018-11-02 01:34:...|2018|   11|     83| free|   null|     null|        82|         Lubbock, TX|\"Mozilla/5.0 (Mac...|\n",
      "|         15|2018-11-02 02:42:...|2018|   11|     66| free|   null|     null|       153|Harrisburg-Carlis...|\"Mozilla/5.0 (Mac...|\n",
      "|         16|2018-11-02 03:05:...|2018|   11|     48| free|   null|     null|        47|         Salinas, CA|\"Mozilla/5.0 (Mac...|\n",
      "|         17|2018-11-02 03:34:...|2018|   11|     86| free|   null|     null|       170|La Crosse-Onalask...|\"Mozilla/5.0 (Mac...|\n",
      "|         18|2018-11-02 05:15:...|2018|   11|     17| free|   null|     null|       118|Chicago-Napervill...|\"Mozilla/5.0 (Win...|\n",
      "|         19|2018-11-02 05:52:...|2018|   11|     66| free|   null|     null|       187|Harrisburg-Carlis...|\"Mozilla/5.0 (Mac...|\n",
      "|         20|2018-11-02 09:01:...|2018|   11|     15| paid|   null|     null|       172|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "+-----------+--------------------+----+-----+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('''\n",
    "#SELECT ROW_NUMBER() OVER(ORDER BY e.ts) AS songplay_id\n",
    "#     , t.start_time\n",
    "#     , EXTRACT(year FROM t.start_time) AS year\n",
    "#     , EXTRACT(month FROM t.start_time) AS month\n",
    "#     , CAST(e.userId AS short) AS user_id\n",
    "#     , e.level\n",
    "#     , s.song_id\n",
    "#     , s.artist_id\n",
    "#     , e.sessionId AS session_id\n",
    "#     , e.location\n",
    "#     , e.userAgent AS user_agent\n",
    "#  FROM staging_events e\n",
    "#       LEFT JOIN staging_songs s\n",
    "#              ON e.song = s.title\n",
    "#                 AND e.artist = s.artist_name\n",
    "#       LEFT JOIN (SELECT ts\n",
    "#                       , to_timestamp(ts/1000.0) AS start_time\n",
    "#                    FROM staging_events\n",
    "#                 ) t\n",
    "#              ON e.ts = t.ts\n",
    "#'''\n",
    "#).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "#song_df = \n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = spark.sql('''\n",
    "SELECT ROW_NUMBER() OVER(ORDER BY e.ts) AS songplay_id\n",
    "     , t.start_time\n",
    "     , CAST(e.userId AS short) AS user_id\n",
    "     , e.level\n",
    "     , s.song_id\n",
    "     , s.artist_id\n",
    "     , e.sessionId AS session_id\n",
    "     , e.location\n",
    "     , e.userAgent AS user_agent\n",
    "     , EXTRACT(year FROM t.start_time) AS year\n",
    "     , EXTRACT(month FROM t.start_time) AS month\n",
    "  FROM staging_events e\n",
    "       LEFT JOIN staging_songs s\n",
    "              ON e.song = s.title\n",
    "                 AND e.artist = s.artist_name\n",
    "       LEFT JOIN (SELECT ts\n",
    "                       , to_timestamp(ts/1000.0) AS start_time\n",
    "                    FROM staging_events\n",
    "                 ) t\n",
    "              ON e.ts = t.ts\n",
    " LIMIT 1\n",
    "'''\n",
    ")\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode('overwrite').partitionBy('year', 'month').parquet(output_data + 'songplays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Songplays Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- songplay_id: integer (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- user_id: short (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: short (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(output_data + 'songplays')\n",
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|songplay_id|          start_time|user_id|level|song_id|artist_id|session_id|            location|          user_agent|year|month|\n",
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|          1|2018-11-01 21:01:...|      8| free|   null|     null|       139|Phoenix-Mesa-Scot...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
